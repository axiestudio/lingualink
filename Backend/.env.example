# ðŸš€ LinguaLink AI Backend - Environment Configuration
# Copy this file to .env and customize the values

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false

# CORS Configuration (add your frontend URLs)
ALLOWED_ORIGINS=["http://localhost:3000", "http://localhost:3001", "https://yourdomain.com"]

# Model Configuration
MODEL_NAME=facebook/nllb-200-distilled-600M
MODEL_CACHE_DIR=./models
DEVICE=auto  # auto, cpu, cuda, mps
MAX_LENGTH=512

# Performance Configuration
BATCH_SIZE=1
NUM_WORKERS=1
ENABLE_CACHING=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# Memory Management
TORCH_COMPILE=false
LOW_MEMORY_MODE=false

# API Configuration
MAX_TEXT_LENGTH=5000
RATE_LIMIT_PER_MINUTE=100

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT="%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Optional: Advanced GPU Settings
# CUDA_VISIBLE_DEVICES=0
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Optional: Hugging Face Hub Token (for private models)
# HUGGINGFACE_HUB_TOKEN=your_token_here
